# -*- coding: utf-8 -*-
"""Hearth_deseases_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BkS0jChrGE1J3qHr4bPuXTS8aez6p_uI
"""

#Various importings I will need during the running
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.feature_selection import SequentialFeatureSelector,SelectKBest,f_classif
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor, BaggingClassifier, StackingClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report ,mean_absolute_error, mean_squared_error, r2_score
import lightgbm as lgb
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
import torch

import warnings
warnings.filterwarnings('ignore')

"""1. LOAD THE DATASET AND DATA PREPROCESSING"""

#to run the project, need to upload the kaggle.json file from the repository
from google.colab import files
files.upload()

import os,zipfile
os.makedirs("/root/.kaggle", exist_ok=True)
os.rename("kaggle.json", "/root/.kaggle/kaggle.json")

os.chmod("/root/.kaggle/kaggle.json", 600)

!kaggle datasets download -d redwankarimsony/heart-disease-data
with zipfile.ZipFile("heart-disease-data.zip", "r") as zip_ref:
    zip_ref.extractall("heart-disease-data")

#Visualize the dataset
df = pd.read_csv("/content/heart-disease-data/heart_disease_uci.csv")
df.head()

#Dropping columns id and dataset, this columns don't represent a useful feature for us
df=df.drop(columns=["id","dataset"], axis=1)

"""Preliminary visualization, to see how target and classes are unbalanced"""

target_balance=df['num'].value_counts()
print(target_balance)

#distribution of classes, very unbalanced for multiclass clasisfication
plt.figure(figsize=(6,4))
sns.countplot(data=df,x='num')
plt.xticks([0,1,2,3,4],['absent','mild','moderate','strong','severe'])
plt.xlabel('Heart Disease')
plt.ylabel('Count')
plt.title('Distribution of classes(Target: heart deseases)')
plt.show()

#Distribution of ages
plt.figure(figsize=(8,5))
sns.histplot(df['age'], kde=True, bins=20)
plt.title('Distribution of ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

#Distribution of heart deseases w.r.t ages
plt.figure(figsize=(8,5))
sns.histplot(data=df, x='age', hue='num', bins=20, kde=True, multiple='stack')
plt.title('Distribution of heart deseases w.r.t ages')
plt.xlabel('Age')
plt.ylabel('Number of cases')
plt.legend(title='Disease', labels=['absent', 'mild','moderate','strong','severe'])
plt.show()

"""2. Fill empty values and one-hot encode"""

numerical_features = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']
categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
n_components=15
# Correctly impute missing values in numerical columns
imputer_numerical = SimpleImputer(strategy="mean")
df[numerical_features] = imputer_numerical.fit_transform(df[numerical_features])

# Correctly impute missing values in categorical columns with the most frequent value
imputer_categorical = SimpleImputer(strategy="most_frequent")
df[categorical_features] = imputer_categorical.fit_transform(df[categorical_features])


# One-hot encode categorical columns
df = pd.get_dummies(df, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'], drop_first=True)


#check if any missing value is still present
(df.isnull().sum()).sort_values(ascending=False)

#Prepare the datasets, for binary and multiclass classification
X=df.drop(columns=['num'], axis=1)
y_5_classes = df['num']
#To simplify, we are gonna do a binary classification as main task
df['num'] = df['num'].apply(lambda x: 1 if x > 0 else 0)
y=df['num']



#Split the dataset in train e test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24)
X_Tr_5, X_Te_5, y_Tr_5, y_Te_5 = train_test_split(X, y_5_classes, test_size=0.25, random_state=24)

"""MODEL #1: K-NN

We are gonna to try the K-NN model with different hyperparameters and choosing the best one


"""

#Split the training set in training e validation, we need the validation set only for K-NN to decide which k is the best one
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=32)
X_Tr_5, X_Val_5, y_Tr_5, y_Val_5 = train_test_split(X_Tr_5, y_Tr_5, test_size=0.25, random_state=32)

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features) # Scale numerical features
    ],
    remainder='passthrough' # Keep the one-hot encoded categorical columns
)


k_values = [3, 5, 7, 9, 15, 22, 35]

def run_knn(X_train, y_train, X_val, y_val, X_test, y_test, k_values, F_selection, B_class, Run_PCA):
  best_k = None
  best_accuracy = 0

  pca='No'
  f_sel='No'
  for k in k_values:
    if(F_selection):
      f_sel='Yes'
      sfs = SequentialFeatureSelector(KNeighborsClassifier(n_neighbors=k), n_features_to_select=10)
      sfs.fit(X_train, y_train)
      selected_features = X.columns[sfs.get_support()]
      X_train_selected = X_train[selected_features]
      X_val_selected = X_val[selected_features]
      X_test_selected = X_test[selected_features]
      numerical_selected=[f for f in numerical_features if f in selected_features]

      preprocessor_selected = ColumnTransformer(
      transformers=[
          ('num', StandardScaler(), numerical_selected) # Scale numerical features
      ],
      remainder='passthrough' # Keep the one-hot encoded categorical columns
      )

    if(F_selection):
      pipeline = Pipeline(steps=[('preprocessor', preprocessor_selected),
                                ('classifier', KNeighborsClassifier(n_neighbors=k))])
      pipeline.fit(X_train_selected, y_train)
      accuracy = pipeline.score(X_val_selected, y_val)
    elif(Run_PCA):
            pca='Yes'
            pipeline=Pipeline(steps=[('preprocessor', preprocessor),
                                     ('pca', PCA(n_components=n_components, svd_solver='full')),
                                    ('classifier', KNeighborsClassifier(n_neighbors=k))])
            pipeline.fit(X_train, y_train)
            accuracy = pipeline.score(X_val, y_val)

    else:
      pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                ('classifier', KNeighborsClassifier(n_neighbors=k))])
      pipeline.fit(X_train, y_train)
      accuracy = pipeline.score(X_val, y_val)




    if accuracy > best_accuracy:
        best_k = k
        best_accuracy = accuracy
  #then create the final pipeline
  if(F_selection):
    final_pipeline = Pipeline(steps=[('preprocessor', preprocessor_selected),
                                ('classifier', KNeighborsClassifier(n_neighbors=best_k))])
    final_pipeline.fit(X_train_selected, y_train)

    test_acc = final_pipeline.score(X_test_selected, y_test)
    y_pred=final_pipeline.predict(X_test_selected)
  elif (Run_PCA):
            final_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                ('pca', PCA(n_components=n_components, svd_solver='full')),
                                ('classifier', KNeighborsClassifier(n_neighbors=best_k))])
            final_pipeline.fit(X_train, y_train)
            test_acc = final_pipeline.score(X_test, y_test)
            y_pred=final_pipeline.predict(X_test)
  else:
    final_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                ('classifier', KNeighborsClassifier(n_neighbors=best_k))])
    final_pipeline.fit(X_train, y_train)

    test_acc = final_pipeline.score(X_test, y_test)
    y_pred=final_pipeline.predict(X_test)

  #test the model and append results
  if(B_class):
    p_score=precision_score(y_test, y_pred)
    rc_score=recall_score(y_test, y_pred)
    f1_sc=f1_score(y_test, y_pred)
    print("Confusion Matrix")
    plt.figure()
    ConfusionMatrixDisplay.from_estimator(final_pipeline, X_test, y_test)
    plt.title(f"Confusion Matrix - KNN")
    plt.grid(False)
    plt.show()
    results.append({'Model': 'KNN',
        'PCA': pca,
        'F_Sel':f_sel,
        'Classes': 'Binary',
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': p_score,
        'Recall': rc_score,
        'F1-score': f1_sc})
  else:
    p_score=precision_score(y_test, y_pred, average='weighted')
    rc_score=recall_score(y_test, y_pred, average='weighted')
    f1_sc=f1_score(y_test, y_pred, average='weighted')

    results.append({'Model': 'KNN',
        'PCA': pca,
        'F_Sel':f_sel,
        'Classes': 'Multiclass',
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': p_score,
        'Recall': rc_score,
        'F1-score': f1_sc})

"""MODEL #2: RANDOM FOREST

We are gonna do a comparison between binary and multiclass classification tasks, and a comparison between feature reduction and not performing it
"""

#define a random forest classifier with 100 trees
clf = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)
def run_rf(X_train, y_train, X_test, y_test, F_selection, B_class):
  if(not F_selection):
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    print("MODEL PERFORMANCE WITH BINARY CLASSIFICATION")
    if(B_class):
      classif='BINARY'
      p_score=precision_score(y_test, y_pred)
      rc_score=recall_score(y_test, y_pred)
      f1_sc=f1_score(y_test, y_pred)
      print("Confusion Matrix")
      plt.figure()
      ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)
      plt.title(f"Confusion Matrix - Random forest")
      plt.grid(False)
      plt.show()
    else:
      classif='MULTICLASS'
      p_score=precision_score(y_test, y_pred, average='weighted')
      rc_score=recall_score(y_test, y_pred, average='weighted')
      f1_sc=f1_score(y_test, y_pred, average='weighted')

    results.append({'Model': 'RF',
      'PCA': 'No',
      'F_Sel': 'No',
      'Classes': classif,
      'Accuracy': accuracy_score(y_test, y_pred),
      'Precision': p_score,
      'Recall': rc_score,
      'F1-score': f1_sc})
    accuracy = accuracy_score(y_test, y_pred)

    #And for each model see the plot with the feature importance
    feat_imp = pd.DataFrame({
      'feature': X_train.columns,
      'importance': clf.feature_importances_
    }).sort_values(by='importance', ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(data=feat_imp.head(10), y='feature', x='importance')
    plt.title("Random Forest Feature Importance")
    plt.show()
  else:
    sfs = SelectKBest(f_classif, k=10)
    sfs.fit(X_train, y_train)
    X_train_selected = sfs.transform(X_train)
    X_test_selected = sfs.transform(X_test)

    selected_columns = X.columns[sfs.get_support()] # Prendi i nomi delle features selezionate

    clf.fit(X_train_selected, y_train)

    y_pred = clf.predict(X_test_selected)
    if(B_class):
      classif='BINARY'
      p_score=precision_score(y_test, y_pred)
      rc_score=recall_score(y_test, y_pred)
      f1_sc=f1_score(y_test, y_pred)
      print("Confusion Matrix")
      plt.figure()
      ConfusionMatrixDisplay.from_estimator(clf, X_test_selected, y_test)
      plt.title(f"Confusion Matrix - Random forest")
      plt.grid(False)
      plt.show()
    else:
      classif='MULTICLASS'
      p_score=precision_score(y_test, y_pred, average='weighted')
      rc_score=recall_score(y_test, y_pred, average='weighted')
      f1_sc=f1_score(y_test, y_pred, average='weighted')

    results.append({'Model': 'RF',
      'PCA': 'No',
      'F_Sel': 'Yes',
      'Classes': classif,
      'Accuracy': accuracy_score(y_test, y_pred),
      'Precision': p_score,
      'Recall': rc_score,
      'F1-score': f1_sc})

#define model and Feature selector for the run with FS

sfs = SequentialFeatureSelector(SVC(kernel='rbf',C=1.0, gamma='scale'), n_features_to_select=10)


preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features) # Scale numerical features
    ],
    remainder='passthrough' # Keep the one-hot encoded categorical columns
)

svm_pipeline =Pipeline(steps=[('preprocessor', preprocessor),
                                ('classifier', SVC(kernel='rbf',C=1.0, gamma='scale'))])

svm = SVC(kernel='rbf',C=1.0, gamma='scale')


def run_svm(X_train, y_train, X_test, y_test, F_selection,B_class,Run_PCA):
  #fit
  pca='No'
  f_sel='No'
  if(F_selection):
    f_sel='Yes'
    sfs.fit(X_train, y_train)
    selected_features = X.columns[sfs.get_support()]
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]
    numerical_selected=[f for f in numerical_features if f in selected_features]

    preprocessor_selected = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_selected) # Scale numerical features
        ],
        remainder='passthrough' # Keep the one-hot encoded categorical columns
    )

    svm_pipeline_selected =Pipeline(steps=[('preprocessor', preprocessor_selected),
                                    ('classifier', SVC(kernel='rbf',C=1.0, gamma='scale'))])

    svm_pipeline_selected.fit(X_train_selected, y_train)
    y_pred = svm_pipeline_selected.predict(X_test_selected)
    svm_pipeline_to_use = svm_pipeline_selected
  elif (Run_PCA):
    pca='Yes'
    svm_pipeline_pca=Pipeline(steps=[('preprocessor', preprocessor),
                                ('pca', PCA(n_components=n_components, svd_solver='full')),
                                ('classifier', SVC(kernel='rbf',C=1.0, gamma='scale'))])
    svm_pipeline_pca.fit(X_train, y_train)
    y_pred = svm_pipeline_pca.predict(X_test)
    svm_pipeline_to_use = svm_pipeline_pca
  else:
    svm_pipeline.fit(X_train, y_train)
    y_pred = svm_pipeline.predict(X_test)
    svm_pipeline_to_use = svm_pipeline



  #store and append the results
  if(B_class):
      classif='BINARY'
      p_score=precision_score(y_test, y_pred)
      rc_score=recall_score(y_test, y_pred)
      f1_sc=f1_score(y_test, y_pred)
      print("Confusion Matrix")
      plt.figure()
      ConfusionMatrixDisplay.from_estimator(svm_pipeline_to_use, X_test, y_test)
      plt.title(f"Confusion Matrix - SVM")
      plt.grid(False)
      plt.show()
  else:
      classif='MULTICLASS'
      p_score=precision_score(y_test, y_pred, average='weighted')
      rc_score=recall_score(y_test, y_pred, average='weighted')
      f1_sc=f1_score(y_test, y_pred, average='weighted')

  results.append({'Model': 'SVM',
      'F_Sel':f_sel,
      'PCA': pca,
      'Classes': classif,
      'Accuracy': accuracy_score(y_test, y_pred),
      'Precision': p_score,
      'Recall': rc_score,
      'F1-score': f1_sc})

"""Now we run the various models with and without PCA, for simplicity the boolean variable F_selection will be used also for PCA, although it's not a feature selector"""

results=[]

#Now the final comparison first binary and multiclass without feature selection

run_knn(X_train, y_train, X_val, y_val, X_test, y_test, k_values, False, True, False)
run_knn(X_train, y_train, X_val, y_val, X_test, y_test, k_values, True, True, False)
run_knn(X_Tr_5, y_Tr_5, X_Val_5, y_Val_5, X_Te_5, y_Te_5, k_values, False, False, False)

run_rf(X_train, y_train, X_test, y_test, False, True)
run_rf(X_train, y_train, X_test, y_test, True, True)
run_rf(X_Tr_5, y_Tr_5, X_Te_5, y_Te_5, False, False)

run_svm(X_train, y_train, X_test, y_test, False, True, False)
run_svm(X_train, y_train, X_test, y_test, True, True, False)
run_svm(X_train, y_train, X_test, y_test, False, True, True)
run_svm(X_Tr_5, y_Tr_5, X_Te_5, y_Te_5, False, False, False)

df_results = pd.DataFrame(results).sort_values(by='F1-score', ascending=False)
print(df_results)